---
title: "When Do We Stop Talking About AI?"
date: 2026-02-07T21:00
type: post
schemaVersion: 1
description: "Every disruptive technology eventually becomes invisible. The discourse ends. The modifier drops. When does that happen with AI?"
tags:
  - ai
  - culture
heroImage: /images/posts/when-stop-talking-ai-hero.png
---

I'm tired.

Not of AI — of *talking* about AI. The think pieces. The existential threads. The "everything will change" proclamations followed by the "nothing has changed" rebuttals. The breathless announcements and the weary corrections. The discourse.

You're tired too, right? I can tell because you clicked on a piece called "When Do We Stop Talking About AI?" hoping someone would give you permission to stop.

But here's why I can't stop thinking about it: every previous technological revolution automated what humans did reluctantly. Machines replaced muscles. We built new jobs for minds.

This one automates the minds.

That's why the discourse is more exhausting. We're not debating whether machines will take the hard jobs. We're debating whether the thing we do when we're *not* doing the hard jobs still matters. It's personal in a way the industrial revolution debates never were.

Every disruptive technology eventually becomes invisible. The modifier drops. The thing that was going to "change everything" just becomes… everything.

So when does that happen with AI?

---

Right now we're stuck in loops.

"AI will take my job" → "No, it makes me more productive" → "But if everyone's more productive, fewer people are needed" → "But new jobs will emerge" → "Will they though?" → repeat.

Or: "Look what AI can do!" → "It's wrong half the time" → "The new model is better" → "Still hallucinates" → "But it's improving exponentially" → repeat.

The exhaustion isn't incidental. It's how discourse dies. We don't stop talking about technologies because we've resolved the debates. We stop because we're tired of having them.

---

Every previous technology wave followed this arc, and each one faster than the last.

Electricity took about thirty years. The 1880s were full of "electric" everything — electric lights, electric motors, electric appliances. People worried about electrocution, job displacement, the transformation of factories. By the 1920s, electricity was infrastructure. You didn't advertise "electric-powered" because what else would it be? The internet compressed the same arc into twenty years — from capitalizing "Internet" and hyphenating "e-mail" to every company being an internet company or not being a company. Mobile did it in ten.

If the pattern holds, "AI-powered" will sound as quaint as "internet-enabled" within five to seven years. By the early 2030s, "AI" might feel like "cyber" does today — a retro prefix from a more excitable time.

But there's a reason I'm not confident about the pattern this time.

---

Previous transitions happened at human-thinking speed. We debated the internet's implications using memos and meetings. We figured out mobile through a decade of trial and error. The discourse unfolded at the pace of human cognition and human institutions.

This transition has a thinking partner.

I'm using AI to write this piece about AI. You might be using AI to process it. We're using the disruption to understand the disruption — in real time, at scale, with the tool itself helping us figure out what the tool means.

Fish discussing water.

I don't know what that does to the timeline. It could compress the discourse phase — we reach the implications faster because we have help thinking. It could amplify it — more takes, more angles, more noise, the same circular debates spinning faster. Probably both at once.

What I notice is that it changes the *texture* of the exhaustion. Previous waves wore people out through slow repetition — the same arguments recycled over years until everyone moved on. This one is wearing people out through velocity. The arguments aren't just repetitive; they're accelerating. The think pieces are blurring together not because we've been having the conversation too long but because we're having it too fast.

And there's a strange recursion to it. AI is good enough to generate the discourse about itself. It can write the optimistic take and the pessimistic take and the "nuanced" take that splits the difference. It can produce the exhaustion and the commentary on the exhaustion.

At some point the loop becomes self-sustaining — discourse generating discourse about discourse — and that might be what actually kills it. Not resolution. Not boredom. Saturation.

---

One thing I'm fairly sure about: the discourse ending won't mean the problems end. It'll mean we've normalized them.

The auto workers who lost jobs to robots didn't get them back. We just stopped writing op-eds about it. The creative workers displaced by AI tools won't all find new roles. We'll just stop finding that interesting.

The power concentration, the labor disruption, the questions about what education means when the "knowledge" part is commoditized — these will continue, quietly, no longer novel enough to discuss.

That's not a prediction about AI. It's an observation about how discourse works. The uncomfortable part is noticing it while it happens — watching the normalization in progress, knowing you're going to participate in it, knowing that your own exhaustion is part of the mechanism.

I wrote [elsewhere](/posts/saaspocalypse/) about the specific shape this is taking: Microsoft's new AI exec running teams of mostly models, Salesforce hiring no software engineers in 2025, Anthropic's CEO predicting all code will be AI-written within a year. The fear isn't irrational. And we'll stop talking about it anyway.

---

But here's what I keep circling back to.

Every previous revolution had a retreat position. The industrial revolution automated muscles, so we retreated to minds. Factory workers became office workers. "Learn to think for a living" was the escape route.

This one automates minds. So where do we retreat?

Maybe the retreat is from *execution* to *direction*. If AI handles the how, humans focus on the what and why. The bottleneck shifts from "people who can do things" to "people who know what's worth doing."

That's not necessarily fewer jobs. It's different jobs. Creation gets cheaper. Execution bottlenecks dissolve. Your vision is no longer limited by what you can personally build. Taste becomes the scarce resource because there's infinite content to curate.

In that world, creativity and agency could explode. More people get to direct. More people get to decide. The privilege of "figuring out what to do" — which used to belong to executives and artists and founders — spreads outward.

But I'm not sure everyone wants that.

Some people find meaning in executing well. In craft. In the satisfaction of making something with their hands — or their code, or their prose. Not everyone wants to be a director. And telling a generation of skilled workers that the new job is "having vision" might land as hollow as telling factory workers to "learn to code."

So the real question isn't whether new jobs emerge. It's whether AI democratizes direction — everyone gets to have a vision — or just automates execution while the same few people decide what gets done.

Probably both, unevenly distributed. Like everything.

The moat isn't gone. The moat is moving. And nobody knows where it settles.

---

Here's my guess at the sequence.

The modifier drops first. "AI-generated" becomes "generated." "AI-assisted" becomes just how you work. This is already happening.

Then the absence becomes notable. Instead of flagging AI use, we flag non-AI use. "Made without AI" becomes the artisanal marker, like "hand-crafted" or "organic."

Then something else demands our attention — biotech, climate tech, quantum, something we're not tracking yet — and the discourse doesn't end so much as migrate.

And then the actual changes continue in the background, no longer interesting enough to discuss. The displacement, the concentration, the restructuring — all still happening, just not worth a think piece anymore.

That's not pessimism. That's just the pattern. The discomfort is being awake to it while it's happening and knowing that being awake to it doesn't change anything.

We're probably three to five years from the modifier dropping. Five to seven from "AI" feeling dated as a category. And then we'll be tired of something else, and the questions we couldn't answer will quietly become the conditions we live in.

At some point it will feel like debating whether electricity will catch on — while standing in a room full of lights.

---

The moment we stop talking about AI won't be the moment it disappears. It will be the moment it has reorganized the substrate.

Every technological revolution eventually reshapes the career ladder. Electricity didn't eliminate electricians — it eliminated the novelty of electrification. The skill moved from spectacle to infrastructure. The internet didn't kill engineers — it made "internet strategy" redundant. It became table stakes. Mobile didn't remain a specialization. It became the baseline assumption of product.

AI will follow the same pattern. Right now, we talk about AI like it's a feature. Or a team. Or a layer. Eventually, it won't be. It will be the condition under which competence is measured.

And when that happens, the early rungs of the ladder move. The junior engineer is no longer evaluated on how many lines they can implement. They're evaluated on how well they can reason about output, trace causality, define constraints, and decide what not to ship. Execution compresses. Judgment becomes visible earlier.

The early rungs won't disappear. They'll shift. And whenever the ladder shifts, the way we learn to climb changes with it.

We stop talking about AI the same way we stopped talking about electricity — not because it didn't matter, but because it became the air.

---

*Written in 2026, when "AI" was still a thing you said out loud. When we were still tired but not yet bored.*
