---
title: "When Do We Stop Talking About AI?"
date: 2026-02-07T21:00
type: post
schemaVersion: 1
description: "The specific exhaustion of a generation that can feel the stitch where human thinking and machine fluency got sewn together."
tags:
  - ai
  - culture
heroImage: /images/posts/when-stop-talking-ai-hero.png
---

This is the third major revision of this essay in a week.

The first draft came fast — clean structure, solid analogies, a confident arc from observation to insight. It sounded right. The problem was I couldn't tell if it was what I actually thought or just what a good essay about AI sounds like. So I rewrote it. And now I'm rewriting it again, trying to push past fluent to honest, which turns out to be where all the real work is.

I'll come back to that. But first, let me lay out the thing that's actually happening — the full shape of it — because I think we keep talking about pieces and missing the picture.

---

Here is everything we are anxious about, all at once.

Every previous technological revolution automated what humans did reluctantly. Machines replaced muscles. We built new jobs for minds. This one automates the minds. There is no retreat position.

The industrial revolution displaced physical laborers, and we told them to learn to think for a living. Now the thinking is getting automated, and nobody has a credible version of "learn to do X instead" because X is the thing being automated.

Fifty-one percent of American workers are worried about losing their jobs to AI this year. Not in the abstract. This year. Entry-level tech hiring at the fifteen largest companies fell twenty-five percent between 2023 and 2024. In the UK, tech graduate roles dropped forty-six percent in a single year, with projections for another fifty-three percent by the end of 2026. Salesforce cut four thousand support roles; their CEO says AI now handles half the company's work. Amazon eliminated fourteen thousand corporate positions. The junior developer pipeline — the traditional first rung of the knowledge-work ladder — is being automated from underneath.

And the discourse about it goes in circles. "AI will take my job" → "No, it makes you more productive" → "But if everyone's more productive, fewer people are needed" → "But new jobs will emerge" → "Will they though?" → repeat.

Or: "Look what AI can do!" → "It's wrong half the time" → "The new model is better" → "Still hallucinates" → "But it's improving exponentially" → repeat.

Meanwhile, Elon Musk and Sam Altman promise abundance — a future where AI generates so much wealth that jobs become optional, where universal basic income fills the gaps. And on the other side, labor economists point out that this is what technologists always promise and it never works out evenly. The wealth concentrates. The displacement scatters. The abundance is real but it accrues to the people who own the machines.

Is it a bubble? The S&P 500 is at its most concentrated in half a century. Five companies hold twenty percent of the global market index. AI valuations look like the dot-com era to some economists, but others note that these companies are actually profitable, unlike 1999's paper unicorns. Sam Altman himself says a bubble is ongoing. Sixty-eight percent of CEOs plan to spend more on AI this year even though less than half of AI projects are paying off. Nobody wants to be the one who didn't invest.

New graduates are entering the worst entry-level market since the pandemic. A bachelor's degree, for the first time in modern history, is no longer a reliable path to professional employment. The traditional deal — trade your early-career grunt work for mentorship and learning — is breaking down because the grunt work is exactly what AI does best. If judgment and taste are what matter now, how do you develop judgment without the years of doing the work that builds it? If we're telling twenty-two-year-olds that the entry point is "knowing what's worth doing," we're asking them to start where people used to end up.

Companies are hiring remote workers in cheaper markets and augmenting them with AI, compressing roles that once took a team of five into a team of two plus a subscription.

The fear isn't irrational. The people who call it overblown aren't lying either. Both things are true at once, which is exactly why the conversation never resolves.

I could keep going. The copyright fights. The environmental cost. The concentration of power in five companies. The open-source versus closed-source wars. The question of what education even means when the knowledge part is commoditized. The creeping suspicion that "AI-powered" is just this decade's "blockchain-enabled" — a buzzword stapled to everything until the correction comes.

All of this is real. All of it is happening simultaneously. And all of it is exhausting to track, not because any single thread is too complex, but because they never resolve and they never stop multiplying.

---

So here's the question I keep coming back to: when does this end? When do we stop talking about AI?

People reach for the historical pattern. Electricity took about thirty years to become invisible. The internet compressed it to twenty. Mobile did it in ten. If the pattern holds, "AI-powered" should sound as quaint as "internet-enabled" within five to seven years.

I don't think the pattern holds. And the reason is almost stupidly simple once you see it.

Every previous technology became invisible because it operated in a different domain than human attention. You don't think about electricity while making toast because electricity works in the domain of physical energy and you think in the domain of cognition. The tool and the attention are in separate lanes. So the tool recedes. The hammer vanishes during hammering. The infrastructure disappears into the act it enables.

AI works in the domain of cognition. Writing, reasoning, analyzing, deciding — the same domain as the thinking you'd use to stop thinking about it. A hammer doesn't resemble the hand that holds it. AI resembles the mind that uses it. And a tool that resembles your own thinking can't become cognitively invisible the way a tool that moves atoms can.

This is why the discourse won't die the way previous tech discourses did. It's not about velocity, or volume, or the number of think pieces. It's that every time you use AI, some part of your attention is doing quality control on the *thinking itself* — is this what I actually mean, or is it what the tool thinks I should mean? Is this my reasoning or a plausible version of my reasoning?

That monitoring is the real fatigue. Not the hot takes. Not the anxiety about jobs or bubbles or concentration of power. Those are real, but they're not why you're *tired*. You're tired because you're using a cognitive tool dozens of times a day and each time there's a micro-negotiation between its fluency and your judgment — between "sounds right" and "is right" — and that negotiation never fully automates because it *is* the thinking.

---

This is the experience I keep having with this essay.

AI gets me to adequate almost instantly. The outline is clean. The analogies are solid. The structure holds. And then I spend days trying to push past adequate to true — past something that sounds like what I think to something that *is* what I think.

The tool is brilliant at producing a plausible version of my idea. The work, the real work, is figuring out what's off about it.

That's not an identity crisis about authorship. It's more mundane than that. It's like editing someone else's draft of your own thoughts — the words are close but the emphasis is wrong, the priorities are slightly shuffled, and you have to keep re-finding your own signal inside something that's *almost* your signal.

It's a specific, repeatable friction. And it doesn't go away with practice. It's there every time.

I think this is what most people experience when they use AI to think or write or decide, even if they don't name it. The feeling of: this is helpful, and also I now have to do a second kind of work I didn't used to have to do — the work of distinguishing the tool's fluency from my own understanding.

With a calculator, you check the output against the input. With AI, you check the output against… yourself. Against something you might not have fully articulated yet, which is precisely why you turned to the tool in the first place.

---

But here's where I have to be honest: I don't think this is permanent.

The seam I'm describing — between my thinking and AI-assisted thinking — depends on having a baseline. I know what my unassisted writing sounds like. I have decades of experience reasoning without a thinking partner, and that experience is what makes the friction detectable.

Take away the baseline and the friction dissolves. Not because the gap between "sounds right" and "is right" closes, but because no one remembers what it felt like to navigate it alone.

Which is exactly what will happen generationally.

Kids growing up with AI as a default collaborator won't experience this seam. They'll never have established a feel for "thinking without AI" any more than they have a feel for "navigating without GPS" or "researching without search engines."

People who grew up with smartphones don't feel the boundary between "online" and "offline" that seemed so fundamental to those of us who remember dial-up. That boundary was real. It shaped an entire decade of discourse. And now it's invisible to a generation that never knew the other side.

AI will follow the same path. The tool won't become invisible by getting better. It'll become invisible because the people using it won't have a "before" to compare it to.

You can't feel a seam if you never knew the fabric was two pieces.

---

So here's what I think is actually happening.

All those anxieties I laid out — the job displacement, the bubble risk, the graduate crisis, the concentration of power, the circular debates — they're real. They're not going away. But they're also not why we're tired.

We're tired because we're the transitional generation.

The ones who can feel the seam between AI-assisted cognition and unassisted cognition, who notice it every time we use the tool, and who can also see that this noticing is temporary. The next generation won't feel it. They'll use AI the way we use search — seamlessly, unremarkably, without the constant micro-friction of checking whether the output matches what we actually believe.

And the specific cruelty of being the transitional generation is that the seam fatigue — the cognitive cost of monitoring the boundary — is consuming the bandwidth we'd need to stay properly alarmed about the structural stuff. The displacement, the concentration, the broken ladder for new graduates.

We can't sustain attention on those problems because the *tool itself* is using up our attention every time we touch it. The very thing reshaping the labor market is also depleting the cognitive resources we'd need to resist that reshaping.

The auto workers who lost jobs to robots in the 1980s didn't get them back. We just stopped writing op-eds about it. The creative workers being displaced now won't all find new roles. We'll stop finding that interesting — not because we decided it was fine, but because our bandwidth ran out.

And part of what drained it was the daily, granular work of using the tool that was doing the displacing.

That's not a conspiracy. It's not even ironic, exactly. It's just what happens when a disruptive technology also happens to be a cognitive tool. It disrupts your ability to sustain attention on the disruption.

The exhaustion is the mechanism.

---

I don't know when the modifier drops. I don't know when "AI" starts to sound like "cyber" — a retro prefix from a more excitable era.

But I think the timeline is set less by the technology maturing and more by the transitional generation cycling out. Fifteen, maybe twenty years. When the people who remember thinking without AI are no longer setting the terms of the conversation, the conversation will end — not because the questions were answered but because no one is left who feels them as questions.

Until then, we're here. Noticing the seam. Doing the work of distinguishing "sounds right" from "is right," dozens of times a day, with a tool that's brilliant at the former and has no access to the latter.

Getting tired not of AI, but of the noticing itself — the constant, low-grade hum of a generation that remembers what thinking felt like before and can't stop comparing.

The anxieties are real. The displacement is real. The bubble risk is real. The broken ladder is real. And the thing that will cause us to stop paying attention to all of it is also real — it's the six-inch seam between human cognition and machine fluency, felt every time we open the tool, invisible to everyone who comes after us.

I don't think there's a name for this yet. Not AI fatigue — that's too broad. Something more specific. *Seam fatigue*, maybe.

The particular exhaustion of a generation that can feel the stitch where human thinking and machine fluency got sewn together, and knows the stitch will be invisible to everyone who comes after.

This is the third draft. I think it's closer now. I'm still not sure.

That's the seam.

*Written in 2026, while the seam was still visible.*
