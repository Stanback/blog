---
title: "The SaaSpocalypse"
description: "Anthropic wiped $285 billion off software stocks this week. The market is panicking about AI replacing tools. But the real disruption is what it means for the people who build them."
date: 2026-02-05
type: post
schemaVersion: 1
draft: false
tags:
  - ai
  - systems
  - judgment
heroImage: /images/posts/saaspocalypse-hero.png
tension: "The market is reacting to the wrong thing. This isn't about tools replacing tools. It's about what happens to us."
preface: "Software stocks just had their worst week since April. The market is probably overreacting. But the anxiety driving the selloff? That's been building for a year. AI is just forcing the conversation into the open."
---

## The $285 Billion Question

On Tuesday, Anthropic's Cowork plugins triggered a [$285 billion selloff](https://www.bloomberg.com/news/articles/2026-02-03/legal-software-stocks-plunge-as-anthropic-releases-new-ai-tool) across software, financial services, and legal tech. Thomson Reuters dropped 15.8%. LegalZoom fell nearly 20%. They're calling it the "SaaSpocalypse."

Is it an overreaction? Probably.

> "Perhaps this is an overreaction, but the threat is real and valuations must account for that."
> — [Reuters](https://www.reuters.com/business/media-telecom/global-software-stocks-hit-by-anthropic-wake-up-call-ai-disruption-2026-02-04/)

When DeepSeek triggered a similar panic last year, Nvidia lost $600 billion in a day. A year later, the feared disruption never materialized. Markets overcorrect. Fear spreads faster than fundamentals.

But here's what's different this time: **the conversation is finally public.**

The anxiety that builders have felt privately for a year—"wait, can AI actually do my job?"—is now being priced into the market. Wall Street is catching up to what anyone using Claude Code already knew: execution is getting cheaper. Fast.

> "AI doesn't make the tough calls on architecture, compliance, or security. It can't fully understand a company's business logic or the ethical implications of a system. That oversight and judgment still rest with experienced engineers."
> — [CIO Magazine](https://www.cio.com/article/4062024/demand-for-junior-developers-softens-as-ai-takes-over.html), September 2025

This is the cope. And it's partially true. But "judgment still rests with humans" glosses over the uncomfortable reality: **judgment is the last moat, and the water is rising.**

---

## The Existential Weight

I've felt this for a year now. A low-grade anxiety that doesn't quite resolve.

It's not fear of being replaced—not exactly. It's something weirder. The sense that the skills I spent a decade building are becoming... not worthless, but *different*. Cheaper. More abundant. The thing that made me valuable is now something I coordinate rather than do.

> "In 2026, writing code is no longer the hard part. AI can generate features, refactor services, and accelerate delivery at scale. Speed is now expected, not a differentiator. What AI removed is friction, not responsibility."
> — [Security Boulevard](https://securityboulevard.com/2026/01/why-senior-software-engineers-will-matter-more-in-2026-in-an-ai-first-world/)

This lands differently when you're living it.

Employment for recent CS graduates has declined 8% since 2022 ([Oxford Economics](https://www.oxfordeconomics.com/wp-content/uploads/2025/05/US-Educated-but-unemployed-a-rising-reality-for-college-grads.pdf)). 90% of tech workers now use AI in their jobs ([Google](https://www.oxfordeconomics.com/wp-content/uploads/2025/05/US-Educated-but-unemployed-a-rising-reality-for-college-grads.pdf)). The funnel that used to produce senior engineers is narrowing at the entry point.

> "38% of engineering leaders fear juniors will get less hands-on experience in AI-heavy workflows."
> — [CodeConductor](https://codeconductor.ai/blog/future-of-junior-developers-ai/)

We're not just changing how software gets built. We're changing who gets to learn how to build it.

---

## How We Adapt

The uncomfortable truth: **the skills that survive are the ones AI can't fake.**

Not code. AI writes code.

Not velocity. AI is faster.

Not consistency. AI doesn't get tired.

What survives:
- **Judgment** — Knowing what to build, not just how
- **Accountability** — Owning outcomes when systems fail
- **Taste** — Recognizing when something is wrong before users do
- **Context** — Understanding the business, the users, the second-order effects

> "Senior engineers don't just execute tasks. They recognize second- and third-order effects. They anticipate failure modes before users discover them. They trade off speed, cost, security, and maintainability deliberately."
> — [Security Boulevard](https://securityboulevard.com/2026/01/why-senior-software-engineers-will-matter-more-in-2026-in-an-ai-first-world/)

This is why multi-agent matters. Not because it makes AI faster—it does—but because it shifts what humans do. You stop writing code. You start coordinating systems that write code.

The question becomes: **are you supervising, or are you being supervised?**

---

## From Vibe Coding to Vibe Working

[Andrej Karpathy coined "vibe coding"](https://x.com/karpathy/status/1886192184808149383) in February 2025: "fully give in to the vibes, embrace exponentials, and forget that the code even exists." Steve Yegge took it further, describing work as fluid—"an uncountable substance that you sling around freely, like slopping shiny fish into wooden barrels at the docks."

> "Some bugs get fixed 2 or 3 times, and someone has to pick the winner. Other fixes get lost. Designs go missing and need to be redone. It doesn't matter, because you are churning forward relentlessly on huge, huge piles of work."
> — [Steve Yegge, "Welcome to Gas Town"](https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04)

This sounds chaotic. It is chaotic. But it's also the emerging reality for anyone running AI at scale.

Now the term is spreading beyond engineering. Anthropic's Scott White [announced Opus 4.6](https://www.cnbc.com/2026/02/05/anthropic-claude-opus-4-6-vibe-working.html) this week with a new framing: **vibe working**.

> "Everybody has seen this transformation happen with software engineering in the last year and a half, where vibe coding started to exist as a concept... I think that we are now transitioning almost into vibe working."

Microsoft embedded the same idea into their [Copilot roadmap](https://www.microsoft.com/en-us/microsoft-365/blog/2025/09/29/vibe-working-introducing-agent-mode-and-office-agent-in-microsoft-365-copilot/)—Agent Mode in Excel and Word, where you describe tasks in plain language and the AI handles the technical work. Brief an agent once, let it run in the background, steer and review.

Salesforce went further: [Agentforce 360](https://www.salesforce.com/news/press-releases/2025/10/13/agentic-enterprise-announcement/) is their platform for "the Agentic Enterprise," and they literally shipped a feature called **Agentforce Vibes**—letting builders "vibe-code" apps grounded in company data. They're reporting 119% agent growth in the first half of 2025.

The shift isn't from "engineer" to "prompt engineer." That's too small. The shift is from **maker** to **orchestrator**. From building to coordinating. From depth to breadth. And it's not just for developers anymore—it's coming for every knowledge worker.

But a year in, the lesson isn't pure chaos. LLMs are genuinely good—and getting better—at greenfield work: new projects, clear requirements, blank slates. They struggle with brownfield: existing codebases, implicit conventions, accumulated context. The answer isn't to reject vibe coding. It's to **harness** it—add constraints, guardrails, structure. Without them, you get [tech debt at AI speed](https://thenewstack.io/5-challenges-with-vibe-coding-for-enterprises/), errors faster than humans can review, and code that works but nobody understands why.

The physics I explore in [[Code Owns Truth]] and [[Proof, Not Truth]] point toward the same conclusion: constraints are the design layer. Prompts express intent. Code owns truth. The vibe is real, but the vibe needs boundaries.

Some people will thrive here. They like systems thinking, coordination, judgment calls. Others will struggle. They liked the craft of code, the satisfaction of a clean implementation, the feeling of having *made* something.

Both reactions are valid. Neither is wrong.

---

## The Ecosystem Shakeout

The SaaSpocalypse isn't just about legal and financial software. It's about every tool built on the assumption that humans do the work.

### Ticketing: Adapt or Replace?

The ticketing systems that track human work are racing to become platforms for AI work.

**Atlassian** launched Rovo—AI agents inside Jira that triage tickets and route work. They're treating agents as a feature layer on top of existing workflows.

**Linear** went further with "[Linear for Agents](https://linear.app/agents)"—AI as full workspace members, assigned to issues, @mentioned in comments. The human remains "primary assignee" while the agent is a "contributor." Accountability preserved, execution delegated.

Meanwhile, new tools like **[Beads](https://github.com/steveyegge/beads)** skip the adaptation entirely—built *for* agents from scratch, no legacy assumptions about human workflows.

The question: **Do you adapt existing tools for AI, or build new tools for an AI-first world?** Linear's hybrid might win the transition. AI-native tools might win the destination. Jira's adding AI to human bureaucracy—that's a harder pivot.

### Automation: Zapier vs. n8n vs. Claude Cowork

The workflow automation platforms face an existential question: **What happens when AI can just do the thing?**

Zapier's response: lean into it. They shipped [Agent Skills for Claude](https://zapier.com/blog/zapier-mcp-agent-skills/)—MCP integrations that let Claude trigger Zapier automations across 8,000+ apps. They achieved 89% AI adoption internally with 800+ agents deployed. Their strategy: become the glue between AI and everything else.

**n8n** is betting on hybrid workflows—AI for the intelligence, n8n for the plumbing. Claude generates n8n workflows. n8n connects to everything. The platform becomes an orchestration layer that AI writes to.

**Make** (formerly Integromat) is in a similar position, competing on visual workflow building while racing to add AI capabilities.

But here's the threat: **Claude Cowork doesn't need Zapier.** If the AI can directly access APIs, authenticate with services, and execute multi-step workflows autonomously—why route through a middleman?

The automation platforms survive if they become:
1. **Connectors** — The authentication and API glue that AI uses
2. **Guardrails** — Human-in-the-loop checkpoints for risky operations
3. **Monitoring** — Observability for what agents are doing

They don't survive as "no-code" tools for humans who can't code. That market is evaporating.

### Winners and Losers

**Winners:**
- **Tools that become infrastructure for AI** — Linear (agents as teammates), Zapier (MCP integrations), n8n (workflow orchestration)
- **Core infrastructure AI can't replace** — AWS, GCP, Cloudflare, Fly.io. The compute and network layer isn't going anywhere.
- **Context repositories** — Notion, Confluence, wikis. AI needs business context, decisions, history. These become the knowledge layer agents pull from.
- **Tools that aggregate judgment** — Platforms where human decisions compound (Figma for design, strategic docs)

**Losers:**
- **Single-function SaaS** — If Claude can do your core function, you're a feature now
- **"No-code for humans"** — The target user can now just ask AI
- **Expensive human expertise platforms** — Legal research, financial analysis, anything Cowork plugins demonstrated

**Uncertain:**
- **Observability (Datadog, Sentry)** — Built for human dashboards. Agents just need a pub/sub bus and error capture. Could be replaced, but implementation cost might not be worth it.
- **GitHub/GitLab** — AI needs version control, but Copilot/Codex compete with them too
- **Slack/Teams** — Communication platforms might become agent coordination hubs... or get bypassed entirely
- **Traditional ticketing (Jira)** — Too much legacy, but also too much lock-in to die quickly

The pattern: **tools survive by becoming either infrastructure (AI needs you) or judgment aggregators (humans need you for decisions AI can't make).**

Everything in the middle—tools that automate what AI now does natively—faces compression.

### Who's Winning the Vibe Coding Wars?

The AI coding tools have stratified fast. [85% of developers](https://devecosystem-2025.jetbrains.com/artificial-intelligence) now use AI tools regularly, with Claude Code, Cursor, and Codex fighting for dominance. Each is racing to ship multi-agent orchestration—the ability to spawn and coordinate multiple AI agents on a single task.

The pattern: **Anthropic is winning on integration, Cursor on UX, OpenAI on raw capability, Google on patience.** But every provider wants to be the platform, not just the model. Anthropic [cracked down on third-party harnesses](https://venturebeat.com/technology/anthropic-cracks-down-on-unauthorized-claude-usage-by-third-party-harnesses) last month—the message: flat-rate pricing requires their tools.

For the full breakdown of what's shipping and how to choose, see [[The Multi-Agent Moment]].

### The Subsidy Question

Here's the uncomfortable math: **current AI pricing is subsidized by investor capital, not sustainable economics.**

OpenAI spent [$22 billion in 2025 against $13 billion in revenue](https://fortune.com/2025/11/12/openai-cash-burn-rate-annual-losses-2028-profitable-2030-financial-documents/)—$1.69 for every dollar earned. They project $74 billion in operating losses in 2028 alone, with cumulative cash burn reaching $115 billion through 2029. The bet: hit $200 billion in revenue by 2030 and turn profitable then.

Anthropic is more disciplined. Their cash burn is projected to drop to one-third of revenue in 2026 and just 9% by 2027, with break-even expected in 2028. They're avoiding expensive video and image generation, focusing on corporate customers (80% of revenue).

What does this mean for builders?

**The current pricing is artificially cheap.** API costs reflect what investors are willing to subsidize, not what the compute actually costs. When the subsidy ends—through profitability pressure, funding crunches, or market corrections—prices go up.

**Lock-in gets more expensive over time.** If you build deep dependencies on one provider's cheap API pricing, you're exposed when they need to raise prices. Anthropic's harness crackdown is a preview: subscription arbitrage disappeared overnight.

**The endgame is unclear.** OpenAI is betting on dominance—spend everything to win the market, then monetize. Anthropic is betting on efficiency—reach profitability faster with less risk. Google is betting on integration—bundle AI into existing products. All three could work. All three could fail.

The honest answer: **nobody knows what sustainable AI pricing looks like yet.** We're all building on shifting sand. The companies burning billions are guessing too.

---

## What Becomes Valuable

Here's a frame I keep coming back to: **SaaS is paying for opinions.**

When you buy software, you're not just buying features. You're buying someone else's opinion about how work should flow. Their assumptions about what steps come first, what fields matter, what the happy path looks like. Sometimes those opinions are useful. Sometimes they're constraints that don't fit how you actually work.

AI dissolves those opinions. Instead of adapting to Jira's workflow or Salesforce's data model, you describe what you need and the system adapts to you. The opinionated software layer becomes optional.

So what survives?

**Models** — The reasoning engines themselves. Anthropic, OpenAI, Google. They're the new primitives. Everything else is built on top.

**Data** — Not data "sellers" exactly, but **data sources**. The AI labs are paying real money: Reddit pulled [$203 million in data licensing](https://www.allmo.ai/articles/unbundling-ai-a-list-of-public-training-data-deals-october-2025), News Corp got ~$250 million over five years, OpenAI offers $1-5M per corpus. Shutterstock is pivoting from stock photos to "AI services for model training." The value shifted from selling content to humans to licensing it to machines.

**Infrastructure** — AWS, GCP, Azure, Cloudflare. The compute layer. IaaS doesn't care what runs on it. If anything, AI makes infrastructure *more* valuable—it's compute-hungry and the demand is only growing.

The middle layer—SaaS tools that wrap workflows around human workers—that's what's compressing.

### The Data Paradox

Here's the tension: AI labs are paying unprecedented amounts for training data. But what happens when sources start locking it down?

Reddit went from free API to $60M/year licensing deals. Stack Overflow made their data exclusive to OpenAI, and [users deleted their answers in protest](https://techcrunch.com/2024/05/16/openai-inks-deal-to-train-ai-on-reddit-data/). News sites are blocking AI crawlers. Getty sued for copyright infringement.

The implications cut both ways:

**If data stays open:** AI gets smarter, models improve, the winners are whoever has the best reasoning engine. Data becomes a commodity.

**If data locks down:** We get balkanized AI. Models trained on different corpuses. Quality depends on who cut the best licensing deals. Data becomes a moat.

The honest answer: we don't know which world we're heading toward. The legal frameworks haven't caught up. The economic incentives point toward closure. But the technical reality is that models trained on open data are already out there, and you can't un-train them.

What's clear: **the companies that control valuable data sources—Reddit, Stack Overflow, news archives, scientific journals—have leverage they didn't have before.** Whether they use it to extract rent or build walls, the dynamics are shifting.

---

## The Bigger Picture

The $285 billion selloff isn't about Cowork or Agent Teams or any specific tool. It's about the market finally internalizing what builders have known for a year: **AI changes the economics of knowledge work.**

Software that used to require specialized teams can now be approximated by general-purpose agents. Legal research, financial analysis, code generation—the boundaries are blurring.

The response isn't to panic. It's to ask: **what do I do that AI can't fake?**

For me, it's judgment. Taste. The ability to recognize when something is wrong before I can articulate why. The willingness to own outcomes when systems fail.

These aren't skills you learn from a tutorial. They come from years of building things, shipping things, watching things break. They come from caring about craft even when nobody's watching.

AI makes execution cheap. That makes judgment expensive.

The existential crisis isn't about being replaced. It's about becoming someone new.

---

*The tools for this transition are already shipping. In [[The Multi-Agent Moment]], I break down what's available—Claude's Agent Teams vs. Gas Town vs. the community alternatives—and how to navigate the chaos.*

---

*I'm still figuring this out. If you're feeling the same weight, I'd like to hear how you're navigating it.*
